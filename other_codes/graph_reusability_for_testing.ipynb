{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import from_networkx\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from torch.nn import LSTM\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(G, window_index,visualization_dir):\n",
    "    pos = nx.spring_layout(G)\n",
    "    pagerank = nx.get_node_attributes(G, 'pagerank')\n",
    "    \n",
    "    # Ensure all nodes have a pagerank value, set default if missing\n",
    "    for node in G.nodes():\n",
    "        if node not in pagerank:\n",
    "            pagerank[node] = 0.0  # Default PageRank value\n",
    "    \n",
    "    labels = {node: f'{node}\\nPR: {pagerank[node]:.2f}' for node in G.nodes()}\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(G, pos, with_labels=True, labels=labels, node_size=7000, node_color='skyblue', font_size=10, edge_color='gray')\n",
    "    plt.title(f\"Graph for Window {window_index}\")\n",
    "    output_path = os.path.join(visualization_dir, f'graph_window_{window_index}.png')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob, title,result_dir):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic - {title}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(f'{result_dir}/roc_curve_{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion,title,result_dir, epochs=100):\n",
    "    model.train()\n",
    "    loss_values = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        loss_values.append(avg_loss)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, epochs + 1), loss_values, label='Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss over Epochs {title}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{result_dir}/traing_loss_over_epochs_{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_loader, title,result_dir):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    for data in test_loader:\n",
    "        out = model(data.x, data.edge_index)\n",
    "        prob = out.softmax(dim=1)[:, 1]  # Get probability of class 1\n",
    "        pred = out.argmax(dim=1)\n",
    "        y_true.extend(data.y.tolist())\n",
    "        y_pred.extend(pred.tolist())\n",
    "        y_prob.extend(prob.tolist())\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred,output_dict=True)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix - {title}')\n",
    "    plt.savefig(f'{result_dir}/confusion_matrix_{title}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plot_roc_curve(y_true, y_prob, title,result_dir)\n",
    "    \n",
    "    return cm, report\n",
    "\n",
    "# Save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_graphs(graph_dir):\n",
    "    pyg_data_list = []\n",
    "    for graph_file in sorted(os.listdir(graph_dir)):\n",
    "        if graph_file.endswith('.graphml'):\n",
    "            graph_path = os.path.join(graph_dir, graph_file)\n",
    "            G = nx.read_graphml(graph_path)\n",
    "            \n",
    "            # Convert networkx graph to PyG data object\n",
    "            pyg_data = from_networkx(G, group_node_attrs=['pagerank'])\n",
    "            pyg_data.x = pyg_data.x.float()  # Ensure x is Float\n",
    "            \n",
    "            # Assuming labels are stored in the graph attributes\n",
    "            labels = [G.nodes[node]['label'] for node in G.nodes]\n",
    "            pyg_data.y = torch.tensor(labels, dtype=torch.long)\n",
    "            \n",
    "            pyg_data_list.append(pyg_data)\n",
    "    \n",
    "    return pyg_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.lin(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "\n",
    "def run_GCnn(train_loader,test_loader,title,result_dir):\n",
    "\n",
    "    model = GCNN(in_channels=1, hidden_channels=128, out_channels=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model_dir =  os.path.join(result_dir, \"saved_models\")\n",
    "    title_dir = os.path.join(result_dir, title)\n",
    "    classification_report_dir = os.path.join(result_dir, \"classification_report\")\n",
    "    os.makedirs(title_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(classification_report_dir, exist_ok=True)\n",
    "\n",
    "    train_model(model, train_loader, optimizer, criterion,title,title_dir)\n",
    "    cm, report = evaluate_model(model, test_loader,title,title_dir)\n",
    "\n",
    "\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    time_str = str(time.time()).replace('.','_')\n",
    "    report_df.to_csv(f'{classification_report_dir}/classification_report_{title}_{time_str}.csv', index=True)\n",
    "    \n",
    "\n",
    "    save_model(model, os.path.join(model_dir, f'graph_{title}_model.pth'))\n",
    "    return cm,report,report['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMAggregator(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(LSTMAggregator, self).__init__()\n",
    "#         self.lstm = LSTM(in_channels, out_channels, batch_first=True)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         # Assuming x is of shape [num_nodes, num_features]\n",
    "#         # Reshape x to [num_nodes, 1, num_features] for LSTM\n",
    "#         x = x.unsqueeze(1)\n",
    "#         x, (hn, cn) = self.lstm(x)\n",
    "#         # Flatten the output to [num_nodes, out_channels]\n",
    "#         x = x.squeeze(1)\n",
    "#         return x\n",
    "    \n",
    "\n",
    "\n",
    "# class EGraphSAGE_with_LSTM(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super(EGraphSAGE_with_LSTM, self).__init__()\n",
    "#         self.lstm_agg = LSTMAggregator(in_channels, hidden_channels)\n",
    "#         self.conv1 = SAGEConv(in_channels, hidden_channels, aggr=self.lstm_agg)\n",
    "#         self.conv2 = SAGEConv(hidden_channels, hidden_channels, aggr=self.lstm_agg)\n",
    "#         self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = self.lin(x)\n",
    "#         return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "\n",
    "\n",
    "# def run_SageConv_lstm(train_loader,test_loader,title,result_dir):\n",
    "\n",
    "#     model = EGraphSAGE_with_LSTM(in_channels=1, hidden_channels=128, out_channels=2)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     model_dir =  os.path.join(result_dir, \"saved_models\")\n",
    "#     title_dir = os.path.join(result_dir, title)\n",
    "#     classification_report_dir = os.path.join(result_dir, \"classification_report\")\n",
    "#     os.makedirs(title_dir, exist_ok=True)\n",
    "#     os.makedirs(model_dir, exist_ok=True)\n",
    "#     os.makedirs(classification_report_dir, exist_ok=True)\n",
    "\n",
    "#     train_model(model, train_loader, optimizer, criterion,title,title_dir)\n",
    "#     cm, report = evaluate_model(model, test_loader,title,title_dir)\n",
    "\n",
    "\n",
    "#     report_df = pd.DataFrame(report).transpose()\n",
    "#     time_str = str(time.time()).replace('.','_')\n",
    "#     report_df.to_csv(f'{classification_report_dir}/classification_report_{title}_{time_str}.csv', index=True)\n",
    "    \n",
    "\n",
    "#     save_model(model, os.path.join(model_dir, f'graph_{title}_model.pth'))\n",
    "#     return cm,report,report['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import LSTM, Linear\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "class LSTMAggregator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(LSTMAggregator, self).__init__()\n",
    "        self.lstm = LSTM(in_channels, out_channels, batch_first=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Implement the LSTM aggregation logic here\n",
    "        return x  # Placeholder implementation\n",
    "\n",
    "class EGraphSAGE_with_LSTM(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(EGraphSAGE_with_LSTM, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')  # Specify aggregation method here\n",
    "        self.lstm_agg = LSTMAggregator(hidden_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')\n",
    "        self.linear = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.lstm_agg(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return self.linear(x)\n",
    "    \n",
    "\n",
    "def run_SageConv_lstm(train_loader,test_loader,title,result_dir):\n",
    "\n",
    "    model = EGraphSAGE_with_LSTM(in_channels=1, hidden_channels=128, out_channels=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model_dir =  os.path.join(result_dir, \"saved_models\")\n",
    "    title_dir = os.path.join(result_dir, title)\n",
    "    classification_report_dir = os.path.join(result_dir, \"classification_report\")\n",
    "    os.makedirs(title_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(classification_report_dir, exist_ok=True)\n",
    "\n",
    "    train_model(model, train_loader, optimizer, criterion,title,title_dir)\n",
    "    cm, report = evaluate_model(model, test_loader,title,title_dir)\n",
    "\n",
    "\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    time_str = str(time.time()).replace('.','_')\n",
    "    report_df.to_csv(f'{classification_report_dir}/classification_report_{title}_{time_str}.csv', index=True)\n",
    "    \n",
    "\n",
    "    save_model(model, os.path.join(model_dir, f'graph_{title}_model.pth'))\n",
    "    return cm,report,report['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_execution_flag = input(\"Do u want new execution\")\n",
    "# new_execution_flag\n",
    "def assign_value():\n",
    "    user_input = input(\"Please enter 'yes' or 'no': \").strip().lower()\n",
    "    \n",
    "    if user_input == '1':\n",
    "        return 1\n",
    "    elif user_input == '0':\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter '1' or '0'.\")\n",
    "        return assign_value()\n",
    "    \n",
    "# assigned_value = assign_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_execution_flag = assign_value()\n",
    "if new_execution_flag == 1:\n",
    "    current_time = time.localtime()\n",
    "    folder_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\", current_time)\n",
    "else:\n",
    "    # folder_name = input(\"Input folder name from all_execution_data\")\n",
    "    folder_name = '2024-07-10_15-26-40'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-07-10_15-26-40'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stage3_data_cleaning/v2\\\\type10_label_merged_final_decoded_clean3.xlsx',\n",
       " 'stage3_data_cleaning/v2\\\\type1_label_merged_final_decoded_clean3.xlsx',\n",
       " 'stage3_data_cleaning/v2\\\\type2_label_merged_final_decoded_clean3.xlsx',\n",
       " 'stage3_data_cleaning/v2\\\\type3_label_merged_final_decoded_clean3.xlsx',\n",
       " 'stage3_data_cleaning/v2\\\\type4_label_merged_final_decoded_clean3.xlsx',\n",
       " 'stage3_data_cleaning/v2\\\\type5_label_merged_final_decoded_clean3.xlsx',\n",
       " 'stage3_data_cleaning/v2\\\\type6_label_merged_final_decoded_clean3.xlsx',\n",
       " 'stage3_data_cleaning/v2\\\\type7_label_merged_final_decoded_clean3.xlsx',\n",
       " 'stage3_data_cleaning/v2\\\\type8_label_merged_final_decoded_clean3.xlsx',\n",
       " 'stage3_data_cleaning/v2\\\\type9_label_merged_final_decoded_clean3.xlsx']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_data(data,output_dir,visualization_dir):\n",
    "    print(\"inside preprocess\")\n",
    "\n",
    "data_clean_folder = \"stage3_data_cleaning/v2\"\n",
    "folder_path = data_clean_folder\n",
    "xlsx_files = glob.glob(os.path.join(folder_path, '*.xlsx'))\n",
    "xlsx_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type10_label\n",
      "2074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Reek\\anaconda3\\envs\\uavcan_v4\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (108x2 and 128x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 37\u001b[0m\n\u001b[0;32m     32\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# _,_,acc = run_SageConv(train_loader,test_loader,'SageConv',result_dir)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# _,_,gat_acc = run_GAT(train_loader,test_loader,'GAT',result_dir)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# _,_,transformer_acc = run_GTransformer(train_loader,test_loader,'Transformer',result_dir)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m _,_,sageConv_lstm_acc \u001b[38;5;241m=\u001b[39m\u001b[43mrun_SageConv_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msageConv_lstm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mresult_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# _,_,gcnn_acc =run_GCnn(train_loader,test_loader,'gcnn__graph_loaded',result_dir)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m result_dict[label_key] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# 'GSageConv': acc,\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# 'GAT': gat_acc,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# 'GCNN': gcnn_acc\u001b[39;00m\n\u001b[0;32m     46\u001b[0m }\n",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m, in \u001b[0;36mrun_SageConv_lstm\u001b[1;34m(train_loader, test_loader, title, result_dir)\u001b[0m\n\u001b[0;32m     40\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(model_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     41\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(classification_report_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtitle_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m cm, report \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader,title,title_dir)\n\u001b[0;32m     47\u001b[0m report_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(report)\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "Cell \u001b[1;32mIn[2], line 42\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, optimizer, criterion, title, result_dir, epochs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 42\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     44\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Reek\\anaconda3\\envs\\uavcan_v4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Reek\\anaconda3\\envs\\uavcan_v4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m, in \u001b[0;36mEGraphSAGE_with_LSTM.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_agg(x, edge_index)\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Reek\\anaconda3\\envs\\uavcan_v4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Reek\\anaconda3\\envs\\uavcan_v4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Reek\\anaconda3\\envs\\uavcan_v4\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (108x2 and 128x2)"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "for file_path in xlsx_files:\n",
    "    \n",
    "\n",
    "    if new_execution_flag == 1:\n",
    "        data = pd.read_excel(file_path)\n",
    "        data['can_id'] = data['can_id'].astype(str)\n",
    "    \n",
    "    label_key = os.path.basename(file_path).split('.')[0][0:12]\n",
    "    print(label_key)\n",
    "\n",
    "    output_dir = f\"all_execution_data/{folder_name}/{label_key}/graphs\"\n",
    "    visualization_dir = os.path.join(f\"all_execution_data/{folder_name}/{label_key}\", \"visualizations\")\n",
    "    result_dir =  os.path.join(f\"all_execution_data/{folder_name}/{label_key}\", \"results\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(visualization_dir, exist_ok=True)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    if new_execution_flag == 1:\n",
    "        pyg_data_list = preprocess_data(data,output_dir,visualization_dir)\n",
    "    else:\n",
    "        pyg_data_list = load_saved_graphs(output_dir)\n",
    "\n",
    "    print(len(pyg_data_list))\n",
    "    # break\n",
    "    train_size = int(0.7 * len(pyg_data_list))\n",
    "    train_data = pyg_data_list[:train_size]\n",
    "    test_data = pyg_data_list[train_size:]\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "    # _,_,acc = run_SageConv(train_loader,test_loader,'SageConv',result_dir)\n",
    "    # _,_,gat_acc = run_GAT(train_loader,test_loader,'GAT',result_dir)\n",
    "    # _,_,transformer_acc = run_GTransformer(train_loader,test_loader,'Transformer',result_dir)\n",
    "    _,_,sageConv_lstm_acc =run_SageConv_lstm(train_loader,test_loader,'sageConv_lstm',result_dir)\n",
    "    # _,_,gcnn_acc =run_GCnn(train_loader,test_loader,'gcnn__graph_loaded',result_dir)\n",
    "\n",
    "    result_dict[label_key] = {\n",
    "        # 'GSageConv': acc,\n",
    "        # 'GAT': gat_acc,\n",
    "        # 'GTransformer': transformer_acc,\n",
    "        'GSage_conv_lstm': sageConv_lstm_acc,\n",
    "        # 'GCNN': gcnn_acc\n",
    "    }\n",
    "\n",
    "    # print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_path)\n",
    "label_key = os.path.basename(file_path).split('.')[0][0:12]\n",
    "print(label_key)\n",
    "\n",
    "output_dir = f\"all_execution_data/{folder_name}/{label_key}/graphs\"\n",
    "visualization_dir = os.path.join(f\"all_execution_data/{folder_name}/{label_key}\", \"visualizations\")\n",
    "result_dir =  os.path.join(f\"all_execution_data/{folder_name}/{label_key}\", \"results\")\n",
    "\n",
    "pyg_data_list = load_saved_graphs(output_dir)\n",
    "print(len(pyg_data_list))\n",
    "\n",
    "train_size = int(0.7 * len(pyg_data_list))\n",
    "train_data = pyg_data_list[:train_size]\n",
    "test_data = pyg_data_list[train_size:]\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "model = EGraphSAGE_with_LSTM(in_channels=1, hidden_channels=128, out_channels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a sample train_loader and test_loader\n",
    "# train_data = [Data(x=torch.randn(10, 1), edge_index=torch.tensor([[0, 1], [1, 0]]), y=torch.tensor([0, 1]))]\n",
    "# test_data = [Data(x=torch.randn(10, 1), edge_index=torch.tensor([[0, 1], [1, 0]]), y=torch.tensor([0, 1]))]\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define the run_SageConv_lstm function\n",
    "# def run_SageConv_lstm(train_loader, test_loader, title, result_dir):\n",
    "#     model = EGraphSAGE_with_LSTM(in_channels=1, hidden_channels=128, out_channels=2)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     model.train()\n",
    "#     for epoch in range(2):  # Training for 2 epochs for demonstration\n",
    "#         for data in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             out = model(data.x, data.edge_index)\n",
    "#             loss = criterion(out, data.y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     for data in test_loader:\n",
    "#         out = model(data.x, data.edge_index)\n",
    "#         pred = out.argmax(dim=1)\n",
    "#         correct += pred.eq(data.y).sum().item()\n",
    "#     accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "#     return model, optimizer, accuracy\n",
    "\n",
    "# Run the function to ensure it works\n",
    "# model, optimizer, accuracy = run_SageConv_lstm(train_loader, test_loader, 'sageConv_lstm', './')\n",
    "# accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uavcan_v4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
