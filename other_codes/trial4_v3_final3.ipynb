{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import from_networkx\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = 'stage3_data_cleaning/v2/type1_label_merged_final_decoded_clean3.xlsx'\n",
    "data = pd.read_excel(data_path)\n",
    "data['can_id'] = data['can_id'].astype(str)\n",
    "\n",
    "output_dir = \"can_graphs/v8\"\n",
    "visualization_dir = os.path.join(output_dir, \"visualizations\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(visualization_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimized_pagerank(G, damping_factor=0.7):\n",
    "    N = len(G)\n",
    "    pagerank = {node: 1 / N for node in G}\n",
    "    for _ in range(100):  # Iterate 100 times for convergence\n",
    "        new_pagerank = {}\n",
    "        for node in G:\n",
    "            rank_sum = 0\n",
    "            for neighbor in G.predecessors(node):\n",
    "                weight_sum = sum([G[neighbor][succ]['weight'] for succ in G.successors(neighbor)])\n",
    "                rank_sum += pagerank[neighbor] * (G[neighbor][node]['weight'] / weight_sum)\n",
    "            new_pagerank[node] = (1 - damping_factor) / N + damping_factor * rank_sum\n",
    "        pagerank = new_pagerank\n",
    "    nx.set_node_attributes(G, pagerank, 'pagerank')\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(window_df):\n",
    "    G = nx.DiGraph()\n",
    "    index_tracker = {}\n",
    "    \n",
    "    for i in range(len(window_df) - 1):\n",
    "        node1 = window_df.iloc[i]['can_id']\n",
    "        node2 = window_df.iloc[i + 1]['can_id']\n",
    "        timestamp_diff = window_df.iloc[i + 1]['timestamp'] - window_df.iloc[i]['timestamp']\n",
    "        label = window_df.iloc[i]['label']\n",
    "        transfer_id1 = window_df.iloc[i]['transfer_ID']\n",
    "        transfer_id2 = window_df.iloc[i + 1]['transfer_ID']\n",
    "        \n",
    "        if node1 != node2 or transfer_id1 != transfer_id2:  # Avoid self-loops\n",
    "            if G.has_edge(node1, node2):\n",
    "                G[node1][node2]['weight'] += timestamp_diff\n",
    "            else:\n",
    "                G.add_edge(node1, node2, weight=timestamp_diff)\n",
    "        \n",
    "        if node1 not in index_tracker:\n",
    "            index_tracker[node1] = []\n",
    "        index_tracker[node1].append((i, label))\n",
    "        \n",
    "        # if i == len(window_df)-1:\n",
    "        if node2 not in index_tracker:\n",
    "            index_tracker[node2] = []\n",
    "        index_tracker[node2].append((i, label))\n",
    "\n",
    "    index_tracker = {k: sorted(list(v)) for k, v in index_tracker.items()}\n",
    "    \n",
    "    # print(index_tracker)\n",
    "    # Convert sets to sorted lists to ensure consistent ordering\n",
    "    index_tracker = {k: sorted(list(v)) for k, v in index_tracker.items()}\n",
    "    \n",
    "    # Calculate optimized PageRank\n",
    "    G = calculate_optimized_pagerank(G)\n",
    "    \n",
    "    return G, index_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the graph and save to file\n",
    "def visualize_graph_old_v1(G, window_index):\n",
    "    pos = nx.spring_layout(G)\n",
    "    pagerank = nx.get_node_attributes(G, 'pagerank')\n",
    "    # indegree = nx.get_node_attributes(G, 'indegree')\n",
    "    labels = {node: f'{node}\\nPR: {pagerank[node]:.2f}\\nInDeg: {indegree[node]}' for node in G.nodes()}\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(G, pos, with_labels=True, labels=labels, node_size=7000, node_color='skyblue', font_size=10, edge_color='gray')\n",
    "    plt.title(f\"Graph for Window {window_index}\")\n",
    "    output_path = os.path.join(visualization_dir, f'graph_window_{window_index}.png')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def visualize_graph(G, window_index):\n",
    "    pos = nx.spring_layout(G)\n",
    "    pagerank = nx.get_node_attributes(G, 'pagerank')\n",
    "    \n",
    "    # Ensure all nodes have a pagerank value, set default if missing\n",
    "    for node in G.nodes():\n",
    "        if node not in pagerank:\n",
    "            pagerank[node] = 0.0  # Default PageRank value\n",
    "    \n",
    "    labels = {node: f'{node}\\nPR: {pagerank[node]:.2f}' for node in G.nodes()}\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(G, pos, with_labels=True, labels=labels, node_size=7000, node_color='skyblue', font_size=10, edge_color='gray')\n",
    "    plt.title(f\"Graph for Window {window_index}\")\n",
    "    output_path = os.path.join(visualization_dir, f'graph_window_{window_index}.png')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def preprocess_data(data, window_size=100):\n",
    "    pyg_data_list = []\n",
    "    for window_start in tqdm(range(0, len(data), window_size)):\n",
    "        window_end = min(window_start + window_size, len(data))\n",
    "        window_data = data.iloc[window_start:window_end]\n",
    "        G, index_tracker = create_graph(window_data)\n",
    "        \n",
    "        # Convert networkx graph to PyG data object\n",
    "        pyg_data = from_networkx(G, group_node_attrs=['pagerank'])\n",
    "        pyg_data.x = pyg_data.x.float()  # Ensure x is Float\n",
    "        \n",
    "        \n",
    "        # Add labels to PyG data object\n",
    "        labels = []\n",
    "        for node in G.nodes:\n",
    "            # Use the most recent label for each node\n",
    "            labels.append(index_tracker[node][-1][1])\n",
    "        pyg_data.y = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Save the raw graph for later analysis\n",
    "        graph_path = os.path.join(output_dir, f'graph_window_{window_start // window_size}.graphml')\n",
    "        nx.write_graphml(G, graph_path)\n",
    "        \n",
    "        # Save the visualization of the graph\n",
    "        visualize_graph(G, window_start // window_size)\n",
    "        \n",
    "        pyg_data_list.append(pyg_data)\n",
    "    \n",
    "    return pyg_data_list\n",
    "\n",
    "\n",
    "\n",
    "# def preprocess_data(data, window_size=100):\n",
    "#     pyg_data_list = []\n",
    "#     for window_start in tqdm(range(0, len(data), window_size)):\n",
    "#         window_end = min(window_start + window_size, len(data))\n",
    "#         window_data = data.iloc[window_start:window_end]\n",
    "#         G, index_tracker = create_graph(window_data)\n",
    "#         # break\n",
    "#         # Convert networkx graph to PyG data object\n",
    "#         pyg_data = from_networkx(G, group_node_attrs=['pagerank'])\n",
    "\n",
    "#         pyg_data.x = pyg_data.x.float()\n",
    "        \n",
    "#         # Add labels to PyG data object\n",
    "#         labels = []\n",
    "#         for node in G.nodes:\n",
    "#             # Use the most recent label for each node\n",
    "#             labels.append(index_tracker[node][-1][1])\n",
    "#         pyg_data.y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "#         # Save the raw graph for later analysis\n",
    "#         graph_path = os.path.join(output_dir, f'graph_window_{window_start // window_size}.gpickle')\n",
    "#         # nx.write_gpickle(G, graph_path)\n",
    "#         nx.write_graphml(G, graph_path)\n",
    "        \n",
    "#         # Save the visualization of the graph\n",
    "#         visualize_graph(G, window_start // window_size)\n",
    "        \n",
    "        \n",
    "#         pyg_data_list.append(pyg_data)\n",
    "    \n",
    "#     return pyg_data_list\n",
    "\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "    \n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for data in test_loader:\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        y_true.extend(data.y.tolist())\n",
    "        y_pred.extend(pred.tolist())\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    return cm, report\n",
    "\n",
    "# Save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "\n",
    "class EGraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.lin(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2079 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2079/2079 [02:43<00:00, 12.69it/s]\n"
     ]
    }
   ],
   "source": [
    "pyg_data_list = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.302394542357196\n",
      "Epoch 2, Loss: 0.23712661635616553\n",
      "Epoch 3, Loss: 0.2260263686918694\n",
      "Epoch 4, Loss: 0.21370285328315652\n",
      "Epoch 5, Loss: 0.21267390380734982\n",
      "Epoch 6, Loss: 0.21187588928834253\n",
      "Epoch 7, Loss: 0.206527818966171\n",
      "Epoch 8, Loss: 0.20887750387191772\n",
      "Epoch 9, Loss: 0.20584902879984482\n",
      "Epoch 10, Loss: 0.20111454796531927\n",
      "Epoch 11, Loss: 0.21092909266767296\n",
      "Epoch 12, Loss: 0.20280337576632915\n",
      "Epoch 13, Loss: 0.2064149243676144\n",
      "Epoch 14, Loss: 0.20343439604925073\n",
      "Epoch 15, Loss: 0.20026986884034198\n",
      "Epoch 16, Loss: 0.2019865843264953\n",
      "Epoch 17, Loss: 0.20383687155402225\n",
      "Epoch 18, Loss: 0.19858395972329637\n",
      "Epoch 19, Loss: 0.20254863685239916\n",
      "Epoch 20, Loss: 0.19529277507377707\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_size = int(0.7 * len(pyg_data_list))\n",
    "train_data = pyg_data_list[:train_size]\n",
    "test_data = pyg_data_list[train_size:]\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "model = EGraphSAGE(in_channels=1, hidden_channels=128, out_channels=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1117  136]\n",
      " [  40  344]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93      1253\n",
      "           1       0.72      0.90      0.80       384\n",
      "\n",
      "    accuracy                           0.89      1637\n",
      "   macro avg       0.84      0.89      0.86      1637\n",
      "weighted avg       0.91      0.89      0.90      1637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm, report = evaluate_model(model, test_loader)\n",
    "print('Confusion Matrix:\\n', cm)\n",
    "print('Classification Report:\\n', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, os.path.join(output_dir, 'graphsage_model_optimized_pagerank_no_indegree.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "class GCNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.lin(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = GCNN(in_channels=1, hidden_channels=128, out_channels=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6961166158966396\n",
      "Epoch 2, Loss: 0.6960158309210902\n",
      "Epoch 3, Loss: 0.6961866293264471\n",
      "Epoch 4, Loss: 0.6962129875369694\n",
      "Epoch 5, Loss: 0.696018108855123\n",
      "Epoch 6, Loss: 0.6955929074598395\n",
      "Epoch 7, Loss: 0.6959004596523617\n",
      "Epoch 8, Loss: 0.696125618789507\n",
      "Epoch 9, Loss: 0.6957598808019058\n",
      "Epoch 10, Loss: 0.6961407700310582\n",
      "Epoch 11, Loss: 0.6963170753872913\n",
      "Epoch 12, Loss: 0.6954345651294874\n",
      "Epoch 13, Loss: 0.6959811578626218\n",
      "Epoch 14, Loss: 0.6966515520344609\n",
      "Epoch 15, Loss: 0.6963633739429972\n",
      "Epoch 16, Loss: 0.6953185187733691\n",
      "Epoch 17, Loss: 0.6959081616090692\n",
      "Epoch 18, Loss: 0.6956511969151704\n",
      "Epoch 19, Loss: 0.6956994533538818\n",
      "Epoch 20, Loss: 0.6959719696770543\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_model(model2, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1117  136]\n",
      " [  40  344]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93      1253\n",
      "           1       0.72      0.90      0.80       384\n",
      "\n",
      "    accuracy                           0.89      1637\n",
      "   macro avg       0.84      0.89      0.86      1637\n",
      "weighted avg       0.91      0.89      0.90      1637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm, report = evaluate_model(model, test_loader)\n",
    "print('Confusion Matrix:\\n', cm)\n",
    "print('Classification Report:\\n', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uavcan_v4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
